# -*- coding: utf-8 -*-
"""market_basket_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ScFY5Pz6dW04oX2Vv6dguyz_cxiflOkP
"""

import pandas as pd

# Load CSV, skip the first row (headers in your file)
df = pd.read_csv('/content/groceries - groceries.csv', header=1)

# Drop the first column which is 'Item(s)' (counts) if not needed
df = df.drop(columns=df.columns[0])
# Explore
print(df.head())

# Convert dataframe to list of transactions
transactions = []
for i in range(len(df)):
    # Drop NaN and convert to list
    transaction = df.iloc[i].dropna().tolist()
    transactions.append(transaction)

# Check first 5 transactions
print(transactions[:5])

"""**One-Hot Encode Transactions**

Apriori and FP-Growth require a dataframe where each column is an item, and each row shows whether that item is in the transaction (True/False).
"""

from mlxtend.preprocessing import TransactionEncoder
import pandas as pd

# Initialize Transaction Encoder
te = TransactionEncoder()

# Fit and transform the transactions
te_ary = te.fit(transactions).transform(transactions)

# Convert to DataFrame
df_trans = pd.DataFrame(te_ary, columns=te.columns_)

# Check first 5 rows
print(df_trans.head())

"""Find Frequent Itemsets
**Using Apriori**
"""

from mlxtend.frequent_patterns import apriori

# Find frequent itemsets with minimum support of 1% (0.01)
frequent_itemsets = apriori(df_trans, min_support=0.01, use_colnames=True)

# Sort by support
frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False)

# Show top 10 frequent itemsets
print(frequent_itemsets.head(10))

"""Find Frequent Itemsets **Using FP-Growth**"""

from mlxtend.frequent_patterns import fpgrowth

# Find frequent itemsets with FP-Growth
frequent_itemsets_fp = fpgrowth(df_trans, min_support=0.01, use_colnames=True)

# Sort by support
frequent_itemsets_fp = frequent_itemsets_fp.sort_values('support', ascending=False)

# Show top 10 frequent itemsets
print(frequent_itemsets_fp.head(10))

"""Generate Association Rules"""

from mlxtend.frequent_patterns import association_rules

# Generate rules from Apriori itemsets
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.2)

# Sort by lift (strength of association)
rules = rules.sort_values('lift', ascending=False)

# Show top 10 rules
print(rules.head(10))

"""Bar Chart of Top Frequent Items"""

import matplotlib.pyplot as plt

# Filter only single items (1-itemsets)
frequent_items = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) == 1)]

# Sort by support
frequent_items = frequent_items.sort_values('support', ascending=False).head(10)

# Plot
plt.figure(figsize=(10,6))
plt.barh([list(x)[0] for x in frequent_items['itemsets']], frequent_items['support'], color='skyblue')
plt.xlabel('Support')
plt.ylabel('Items')
plt.title('Top 10 Most Frequent Items')
plt.gca().invert_yaxis()  # largest on top
plt.show()

"""Visualize Association Rules with a Network Graph"""

import networkx as nx

# Use only top 10 rules by lift for clarity
top_rules = rules.head(10)

# Create directed graph
G = nx.DiGraph()

for _, row in top_rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'])

# Draw graph
plt.figure(figsize=(10,6))
pos = nx.spring_layout(G, k=1)
nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightgreen', font_size=12, arrowsize=20)
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels={k: round(v,2) for k,v in edge_labels.items()})
plt.title('Top 10 Association Rules by Lift')
plt.show()

"""Scatter plot of Support vs Confidence"""

import seaborn as sns

plt.figure(figsize=(8,6))
sns.scatterplot(x='support', y='confidence', size='lift', data=rules, alpha=0.6)
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.title('Association Rules: Support vs Confidence')
plt.show()